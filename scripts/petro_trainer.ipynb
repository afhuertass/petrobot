{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Activation, Dense\n",
    "import string\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/petrogustavo.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>La bombero del Distrito Zulima Muñoz murió en ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Thu Nov 27 14:50:53 +0000 2014</td>\n",
       "      <td>32</td>\n",
       "      <td>537981895733424130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>363</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>El progresismo será un movimiento independient...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Fri Nov 25 00:32:15 +0000 2016</td>\n",
       "      <td>183</td>\n",
       "      <td>801946567095779328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>545</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Esta es la verdadera causa de la bancarrota de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Sun Nov 26 20:19:03 +0000 2017</td>\n",
       "      <td>374</td>\n",
       "      <td>934879192126287873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>_ @FelippeDuke la experiencia del crédito fue ...</td>\n",
       "      <td>FelipeDuqueM1</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Feb 09 16:18:01 +0000 2016</td>\n",
       "      <td>43</td>\n",
       "      <td>697092093857165312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>El magisterio es una fuerza laboral esencialme...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Mon Oct 05 15:01:19 +0000 2015</td>\n",
       "      <td>58</td>\n",
       "      <td>651049533921718272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   favorite_count               source  \\\n",
       "0              33  Twitter for Android   \n",
       "1             363   Twitter Web Client   \n",
       "2             545  Twitter for Android   \n",
       "3              59  Twitter for Android   \n",
       "4              31  Twitter for Android   \n",
       "\n",
       "                                                text in_reply_to_screen_name  \\\n",
       "0  La bombero del Distrito Zulima Muñoz murió en ...                     NaN   \n",
       "1  El progresismo será un movimiento independient...                     NaN   \n",
       "2  Esta es la verdadera causa de la bancarrota de...                     NaN   \n",
       "3  _ @FelippeDuke la experiencia del crédito fue ...           FelipeDuqueM1   \n",
       "4  El magisterio es una fuerza laboral esencialme...                     NaN   \n",
       "\n",
       "   is_retweet                      created_at  retweet_count  \\\n",
       "0       False  Thu Nov 27 14:50:53 +0000 2014             32   \n",
       "1       False  Fri Nov 25 00:32:15 +0000 2016            183   \n",
       "2       False  Sun Nov 26 20:19:03 +0000 2017            374   \n",
       "3       False  Tue Feb 09 16:18:01 +0000 2016             43   \n",
       "4       False  Mon Oct 05 15:01:19 +0000 2015             58   \n",
       "\n",
       "               id_str  \n",
       "0  537981895733424130  \n",
       "1  801946567095779328  \n",
       "2  934879192126287873  \n",
       "3  697092093857165312  \n",
       "4  651049533921718272  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply( lambda x :  x.lower() )\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x:  re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x:  re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE))\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x:   unidecode.unidecode(x )   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "characters = list(string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(string.printable)\n",
    "characters.remove('\\x0b')\n",
    "characters.remove('\\x0c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary len = 98\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', ' ', '\\t', '\\n', '\\r']\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_SIZE = len(characters)\n",
    "characters_to_ix = {c:i for i,c in enumerate(characters)}\n",
    "print(\"vocabulary len = %d\" % VOCABULARY_SIZE)\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GPU = 1 # you can experiment with more GPUs, it gets interesting with a high SEQUENCE_LEN\n",
    "SEQUENCE_LEN = 140\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 20\n",
    "HIDDEN_LAYERS_DIM = 512\n",
    "LAYER_COUNT = 4\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model( layer_count , sequence_len , vocab_size , dropout ):\n",
    "    hidden_layers_dim = 512\n",
    "    model = Sequential()\n",
    "    for i in range( layer_count ):\n",
    "        \n",
    "        model.add(\n",
    "            LSTM(\n",
    "                hidden_layers_dim , \n",
    "                return_sequences=True if (i!=(layer_count-1)) else False,\n",
    "                input_shape=( sequence_len , vocab_size ),\n",
    "            )\n",
    "        )\n",
    "        model.add( Dropout(dropout))\n",
    "        \n",
    "    model.add(Dense(VOCABULARY_SIZE))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_batch(X, y, samples=3):\n",
    "    \"\"\"Describe in a human-readable format some samples from a batch\"\"\"\n",
    "    for i in range(samples):\n",
    "        sentence = \"\"\n",
    "        for s in range(SEQUENCE_LEN):\n",
    "            sentence += characters[X[i,s,:].argmax()]\n",
    "        next_char = characters[y[i,:].argmax()]\n",
    "        \n",
    "        print(\"sample #%d: ...%s -> '%s'\" % (\n",
    "            i,\n",
    "            sentence[-20:],\n",
    "            next_char\n",
    "        ))\n",
    "\n",
    "def batch_generator(text, count):\n",
    "    \"\"\"Generate batches for training\"\"\"\n",
    "    while True: # keras wants that for reasons\n",
    "        for batch_ix in range(count):\n",
    "            X = np.zeros((BATCH_SIZE, SEQUENCE_LEN, VOCABULARY_SIZE))\n",
    "            y = np.zeros((BATCH_SIZE, VOCABULARY_SIZE))\n",
    "\n",
    "            batch_offset = BATCH_SIZE * batch_ix\n",
    "\n",
    "            for sample_ix in range(BATCH_SIZE):\n",
    "                sample_start = batch_offset + sample_ix\n",
    "                for s in range(SEQUENCE_LEN):\n",
    "                    X[sample_ix, s, characters_to_ix[text[sample_start+s]]] = 1\n",
    "                y[sample_ix, characters_to_ix[text[sample_start+s+1]]]=1\n",
    "\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df[\"text\"].sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14234,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = df_full[:12000]\n",
    "df_test = df_full[12000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \\\\ I will use this indicator of the end of a twitt  \n",
    "full_train_text = \"\\\\\".join( df_train[:] )\n",
    "full_test_text = \"\\\\\".join( df_test[:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len( full_train_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = len( full_test_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_count = (text_train_len - SEQUENCE_LEN) // BATCH_SIZE\n",
    "val_batch_count = (text_val_len - SEQUENCE_LEN) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2824\n",
      "524\n"
     ]
    }
   ],
   "source": [
    "print(train_batch_count)\n",
    "print( val_batch_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model( LAYER_COUNT , SEQUENCE_LEN , VOCABULARY_SIZE , DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/models/petrobot_weights\"\n",
    "#call backs \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    save_weights_only=True\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "callbacks_list = [ checkpoint , early_stopping ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f042f26acc0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = training_model.fit_generator(\n",
    "    batch_generator( full_train_text , count=train_len),\n",
    "    train_len,\n",
    "    max_queue_size=1, # no more than one queued batch in RAM\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=batch_generator( full_test_text , count=test_len ),\n",
    "    validation_steps=test_len,\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
